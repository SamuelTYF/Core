\documentclass[a4paper,12pt]{article}

\usepackage{amsmath}
\usepackage{bm}

\title{SVM}
\date{\today}

\begin{document}
    \maketitle
    \tableofcontents
    \clearpage

    \section{Introduction}

        Sample Space:

        \begin{equation}
            D=\left\lbrace \bm{x}_1,\bm{x}_2,\cdots,\bm{x}_n\right\rbrace
        \end{equation}

        Hyperplane can split Sample Space into two partitions:

        \begin{equation}
            \bm{\omega}^T\bm{x}+b=0
        \end{equation}

        The distance of each point $\bm{x}_i$ from Hyperplane $(\bm{\omega},b)$ can be written as:

        \begin{equation}
            r=\frac{|\bm{\omega}^T\bm{x}+b|}{\|\bm{\omega}\|}
        \end{equation}

        The Decision Rule:

        \begin{equation}
            r=\frac{|\bm{\omega}^T\bm{x}+b|}{\|\bm{\omega}\|}\geq 0\Rightarrow \bm{x}\;is\;positive\;sample
        \end{equation}

        We can use factor to scale Hyperplane to $(\alpha\bm{\omega},\beta b)$ so that the rule of seperate two partitions can be written as (For Mathematical Convenient):

        \begin{equation}
            \label{rule}
            \begin{cases}
                \bm{\omega}^T\bm{x}_i+b\geq +1 & y_i=+1\\
                \bm{\omega}^T\bm{x}_i+b\leq -1 & y_i=-1\\
            \end{cases}
        \end{equation}

        The distance of Surfuce Hyperplanes $(\bm{\omega},b+1)$ from $(\bm{\omega},b-1)$

        \begin{equation}
            \gamma=\frac{2}{\|\omega\|}
        \end{equation}

        Our destination is to maximize the distance in the constrains \ref{rule}

        \begin{equation}
            \begin{split}
                & \max_{\bm{\omega},b} \frac{2}{\|\omega\|}\\
                & y_i(\bm{\omega}^T\bm{x}_i+b)\geq 1
            \end{split}
        \end{equation}

        The Support Vectors satisify:

        \begin{equation}
            y_i(\bm{\omega}^T\bm{x}_i+b)-1=0
        \end{equation}

        It's akin to minimize the norm of $\omega$

        \begin{equation}
            \begin{split}
                & \min_{\bm{\omega},b} \frac{\|\omega\|^2}{2}\\
                & y_i(\bm{\omega}^T\bm{x}_i+b)\geq 1
            \end{split}
        \end{equation}

    \section{Dual}

        Using Lagrange Multiplier Method

        \begin{equation}
            L(\bm{\omega},b,\bm{\alpha})=\frac{\|\bm{\omega}\|^2}{2}+\sum_{i=1}^n\alpha_i[1-y_i(\bm{\omega}^T\bm{x}_i+b)]
        \end{equation}

        \begin{equation}
            \begin{split}
                \frac{\partial L}{\partial \bm{\omega}}&=\bm{\omega}-\sum_{i=1}^n\alpha_iy_i\bm{x}_i=0\\
                \frac{\partial L}{\partial b}&=-\sum_{i=1}^n\alpha_iy_i=0
            \end{split}
        \end{equation}

        \begin{equation}
            L(\bm{\omega},b,\bm{\alpha})=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j
        \end{equation}

        The Dual Form of SVM:

        \begin{equation}
            \begin{split}
                & \min_{\bm{\alpha}} \sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j\\
                & \sum_{i=1}^n\alpha_iy_i=0\\
                & \bm{\alpha}\geq 0
            \end{split}
        \end{equation}

        \begin{equation}
            f(\bm{x})=\bm{\omega}^T\bm{x}+b=\sum_{i}^n\alpha_iy_i\bm{x}_i^T\bm{x}+b
        \end{equation}

        It meets the KKT rules

        \begin{equation}
            \begin{split}
                & \bm{\alpha}\geq 0\\
                & y_if(\bm{x}_i)-1 \geq 0\\
                & \alpha_i(y_if(\bm{x}_i)-1)=0
            \end{split}
        \end{equation}

        When $\alpha_i=0$, $(\bm{x}_i,y_i)$ will be ignored

        When $\alpha_i\ge 0$, $y_if(\bm{x}_i)=1$ which means $(\bm{x}_i,y_i)$ in the Surfuce Hyperplanes.

    \section{SMO}

        Select two variable $\alpha_i$,$\alpha_j$ and fix other variable

        \begin{equation}
            \begin{split}
                & \min_{\alpha_i,\alpha_j} \alpha_i+\alpha_j-\frac{1}{2}\alpha_i^2y_i^2\|\bm{x}_i\|^2-\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j-\frac{1}{2}\alpha_j^2y_j^2\|\bm{x}_j\|^2\\
                & -\sum_{k\neq i,j}\alpha_i\alpha_ky_iy_k\bm{x}_i^T\bm{x}_k-\sum_{k\neq i,j}\alpha_j\alpha_ky_jy_k\bm{x}_j^T\bm{x}_k\\
                & \alpha_iy_i+\alpha_jy_j=\sum_{k\neq i,j}\alpha_ky_k=\varsigma\\
                & \alpha_i \geq 0,\alpha_j \geq 0
            \end{split}
        \end{equation}

        \begin{equation}
            a_j=\varsigma y_j-\alpha_iy_iy_j
        \end{equation}

        Solve the equation

        \begin{equation}
            \begin{split}
                &(\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x}_j+\|\bm{x}_j\|^2)\alpha_i\\
                &=1-y_iy_j-\varsigma y_i\bm{x}_i^T\bm{x}_j+\varsigma y_i\|\bm{x}_j\|^2\\
                &-y_i\sum_{k\neq i,j}\alpha_ky_k\bm{x}_i^T\bm{x}_k+y_i\sum_{k\neq i,j}\alpha_ky_k\bm{x}_j^T\bm{x}_k
            \end{split}
        \end{equation}

        Owing to the complexity, We want to replace $\varsigma$

        \begin{equation}
            \begin{split}
                f(\bm{x}_i)&=\sum_{k}^n\alpha_ky_y\bm{x}_k^T\bm{x_i}+b\\
                &=\alpha_iy_i\|\bm{x}_i\|^2+\alpha_jy_j\bm{x}_j^T\bm{x_i}+\sum_{k\neq i,j}\alpha_ky_k\bm{x}_i^T\bm{x}_k+b\\
                f(\bm{x}_j)&=\sum_{k}^n\alpha_ky_y\bm{x}_k^T\bm{x_j}+b\\
                &=\alpha_jy_j\|\bm{x}_j\|^2+\alpha_iy_i\bm{x}_i^T\bm{x_j}+\sum_{k\neq i,j}\alpha_ky_k\bm{x}_j^T\bm{x}_k+b
            \end{split}
        \end{equation}

        \begin{equation}
            \begin{split}
                &\sum_{k\neq i,j}\alpha_ky_k\bm{x}_j^T\bm{x}_k-\sum_{k\neq i,j}\alpha_ky_k\bm{x}_i^T\bm{x}_k\\
                &=f(\bm{x}_j)-f(\bm{x}_i)\\
                &+\alpha_iy_i\|\bm{x}_i\|^2-\alpha_jy_j\|\bm{x}_j\|^2\\
                &+\alpha_jy_j\bm{x}_j^T\bm{x_i}-\alpha_iy_i\bm{x}_i^T\bm{x_j}
            \end{split}
        \end{equation}

        \begin{equation}
            \begin{split}
                &y_i\left[\sum_{k\neq i,j}\alpha_ky_k\bm{x}_j^T\bm{x}_k-\sum_{k\neq i,j}\alpha_ky_k\bm{x}_i^T\bm{x}_k\right]\\
                &=y_i\left(f(\bm{x}_j)-f(\bm{x}_i)\right)\\
                &+\alpha_i\|\bm{x}_i\|^2-(\varsigma y_j-\alpha_iy_iy_j)y_iy_j\|\bm{x}_j\|^2\\
                &+(\varsigma y_j-\alpha_iy_iy_j)y_iy_j\bm{x}_j^T\bm{x_i}-\alpha_i\bm{x}_i^T\bm{x_j}\\
                &=y_i\left(f(\bm{x}_j)-f(\bm{x}_i)\right)\\
                &+\left(\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x_j}+\|\bm{x}_j\|^2\right)\alpha_i\\
                &+\varsigma y_i\bm{x}_i^T\bm{x_j}-\varsigma y_i\|\bm{x}_j\|^2\\
            \end{split}
        \end{equation}

        \begin{equation}
            \begin{split}
                &(\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x}_j+\|\bm{x}_j\|^2)\alpha_i^*\\
                &=1-y_iy_j-\varsigma y_i\bm{x}_i^T\bm{x}_j+\varsigma y_i\|\bm{x}_j\|^2\\
                &+y_i\left(f(\bm{x}_j)-f(\bm{x}_i)\right)\\
                &+\left(\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x_j}+\|\bm{x}_j\|^2\right)\alpha_i\\
                &+\varsigma y_i\bm{x}_i^T\bm{x_j}-\varsigma y_i\|\bm{x}_j\|^2\\
                &=\left(\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x_j}+\|\bm{x}_j\|^2\right)\alpha_i\\
                &+1-y_iy_j+y_i\left(f(\bm{x}_j)-f(\bm{x}_i)\right)
            \end{split}
        \end{equation}

        \begin{equation}
            \begin{split}
                \alpha_i^*=\alpha_i+\frac{y_i\left[(f(\bm{x}_j)-y_j)-(f(\bm{x}_i)-y_i)\right]}{\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x}_j+\|\bm{x}_j\|^2}\\
                \alpha_j^*=\alpha_j+\frac{y_j\left[(f(\bm{x}_i)-y_i)-(f(\bm{x}_j)-y_j)\right]}{\|\bm{x}_i\|^2-2\bm{x}_i^T\bm{x}_j+\|\bm{x}_j\|^2}
            \end{split}
        \end{equation}

        Consider the range of $\alpha_i$,$\alpha_j$

        \begin{equation}
            \begin{split}
                \alpha_i\in[0,C]\\
                \alpha_j\in[0,C]\\
            \end{split}
        \end{equation}

        When $y_i=y_j$,$\alpha_i+\alpha_j=\varsigma y_i$

        \begin{equation}
            \begin{split}
                \inf{\alpha_i^*}&=\max\left\{0,\alpha_j+\alpha_i-C\right\}\\
                \sup{\alpha_i^*}&=\min\left\{C,\alpha_j+\alpha_i\right\}
            \end{split}
        \end{equation}

        When $y_i\neq y_j$,$\alpha_i-\alpha_j=\varsigma y_i$

        \begin{equation}
            \begin{split}
                \inf{\alpha_i^*}&=\max\left\{0,\alpha_j-\alpha_i\right\}\\
                \sup{\alpha_i^*}&=\min\left\{C,\alpha_j-\alpha_i+C\right\}
            \end{split}
        \end{equation}

        \begin{equation}
            \begin{split}
                \bar{\alpha}_i&=\begin{cases}
                    \sup{\alpha_i^*},\alpha_i^*\geq\sup{\alpha_i^*}\\
                    \alpha_i^*,\inf{\alpha_i^*}\leq\alpha_i^*\leq\sup{\alpha_i^*}\\
                    \inf{\alpha_i^*},\alpha_i^*\leq\inf{\alpha_i^*}
                \end{cases}\\
                \bar{\alpha}_j&=\alpha_j+(\alpha_i-\bar{\alpha}_i) y_iy_j
            \end{split}
        \end{equation}

        After the computation of $\alpha_i$,$\alpha_j$, we need to calculate $b$

        We maintain the Support Vector Set $S$ in the Surface Hyperplanes

        \begin{equation}
            \begin{split}
                (\bm{x}_s,y_s)\in S\\
                y_s(\bm{\omega}^T\bm{x}_s+b)=1\\
                \bm{\omega}=\sum_{i\in S}\alpha_iy_i\bm{x}_i^T
            \end{split}
        \end{equation}

        \begin{equation}
            b+\sum_{i\in S}\alpha_iy_i\bm{x}_i^T\bm{x}_s=y_s
        \end{equation}

        \begin{equation}
            \bar{b}=\frac{1}{|S|}\sum_{s\in S}\left(y_s-\sum_{i\in S}\alpha_iy_i\bm{x}_i^T\bm{x}_s\right)
        \end{equation}

        Also

        \begin{equation}
            \bar{b}_i=y_i-\bar{\alpha}_iy_i\|\bm{x}_i\|^2-\bar{\alpha}_jy_j\bm{x}_i^T\bm{x}_j-\sum_{k\neq i,j}^n\alpha_ky_k\bm{x}_k^T\bm{x}_i
        \end{equation}

        \begin{equation}
            \sum_{k\neq i,j}^n\alpha_ky_k\bm{x}_k^T\bm{x}_i=f(\bm{x}_i)-\alpha_iy_i\|\bm{x}_i\|^2-\alpha_jy_j\bm{x}_i^T\bm{x}_j-b
        \end{equation}

        \begin{equation}
            \begin{split}
                \bar{b}_i&=b-(f(\bm{x}_i)-y_i)+(\alpha_i-\bar{\alpha}_i)y_i\|\bm{x}_i\|^2+(\alpha_j-\bar{\alpha}_j)y_j\bm{x}_i^T\bm{x}_j\\
                \bar{b}_j&=b-(f(\bm{x}_j)-y_j)+(\alpha_j-\bar{\alpha}_j)y_j\|\bm{x}_j\|^2+(\alpha_i-\bar{\alpha}_i)y_i\bm{x}_i^T\bm{x}_j
            \end{split}
        \end{equation}

        \begin{equation}
            \bar{b}=\frac{\bar{b}_i+\bar{b}_j}{2}
        \end{equation}

    \section{Kernel Function}

        Transform Function $\phi(\bm{x})$ map the vector $\bm{x}$ into other space

        \begin{equation}
            \begin{split}
                & \min_{\bm{\alpha}} \sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\phi(\bm{x}_i)^T\phi(\bm{x}_j)\\
                & \sum_{i=1}^n\alpha_iy_i=0\\
                & \bm{\alpha}\geq 0
            \end{split}
        \end{equation}

        We can construct the Kernel Function instead of knowning the Transform Function

        \begin{equation}
            K(\bm{x}_i,\bm{x}_j)=\phi(\bm{x}_i)^T\phi(\bm{x}_j)
        \end{equation}

        Kernel Function satisify Mercer Theory

        \begin{equation}
            \kappa=\left\lbrace K(\bm{x}_i,\bm{x}_j) \right\rbrace \succeq 0
        \end{equation}

        Polynomial Kernel

        \begin{equation}
            K(\bm{x}_i,\bm{x}_j)=(\bm{x}_i\bm{x}_j+1)^n
        \end{equation}

        Gaussian Kernel

        \begin{equation}
            K(\bm{x}_i,\bm{x}_j)=e^{-\frac{\|\bm{x}_i-\bm{x}_j\|^2}{2\sigma^2}}
        \end{equation}

        Laplacian Kernel

        \begin{equation}
            K(\bm{x}_i,\bm{x}_j)=e^{-\frac{\|\bm{x}_i-\bm{x}_j\|}{\sigma}}
        \end{equation}

        Sigmoid Kernel

        \begin{equation}
            K(\bm{x}_i,\bm{x}_j)=\tanh(\beta\bm{x}_i^T\bm{x}_j+\theta)
        \end{equation}

        \begin{equation}
            \begin{split}
                &\gamma_1K_1+\gamma_2K_2\\
                &K_1\otimes K_2(\bm{x}_i,\bm{x}_j)=K_1(\bm{x}_i,\bm{x}_j)K_2(\bm{x}_i,\bm{x}_j)\\
                &f(\bm{x}_i)K(\bm{x}_i,\bm{x}_j)f(\bm{x}_j)
            \end{split}
        \end{equation}

    \section{RKHS}

\end{document}